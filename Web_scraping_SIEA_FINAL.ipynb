{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e5811ed",
   "metadata": {},
   "source": [
    "# proyecto Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590f5e97",
   "metadata": {},
   "source": [
    "PS D:\\Estudios_extra\\Big_Data_Python\\Aplicando_WEB_SCRAPING> Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope Process\n",
    "PS D:\\Estudios_extra\\Big_Data_Python\\Aplicando_WEB_SCRAPING> venv_scrap\\Scripts\\activate\n",
    "(venv_scrap) PS D:\\Estudios_extra\\Big_Data_Python\\Aplicando_WEB_SCRAPING> \n",
    "fwd-i-search: _\n",
    "\n",
    "(venv_scrap) PS D:\\Estudios_extra\\Big_Data_Python\\Aplicando_WEB_SCRAPING> # Crear directorios para la estructura del proyecto\n",
    ">> mkdir scraper\\spiders -Force\n",
    ">> mkdir data\\raw -Force\n",
    ">> mkdir data\\processed -Force\n",
    ">> mkdir notebooks -Force\n",
    ">> mkdir tests -Force\n",
    ">> mkdir logs -Force\n",
    "\n",
    "\n",
    "(venv_scrap) PS D:\\Estudios_extra\\Big_Data_Python\\Aplicando_WEB_SCRAPING> # Alternativa más sencilla\n",
    ">> New-Item -Path \"scraper\\__init__.py\" -ItemType \"file\" -Force\n",
    ">> New-Item -Path \"scraper\\spiders\\__init__.py\" -ItemType \"file\" -Force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aefd887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.13.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp311-cp311-win_amd64.whl.metadata (19 kB)\n",
      "Collecting scrapy\n",
      "  Downloading Scrapy-2.12.0-py2.py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting selenium\n",
      "  Using cached selenium-4.31.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting lxml\n",
      "  Downloading lxml-5.3.2-cp311-cp311-win_amd64.whl.metadata (3.7 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests)\n",
      "  Using cached charset_normalizer-3.4.1-cp311-cp311-win_amd64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests)\n",
      "  Downloading urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4)\n",
      "  Downloading soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in d:\\estudios_extra\\big_data_python\\aplicando_web_scraping\\venv_scrap\\lib\\site-packages (from beautifulsoup4) (4.13.2)\n",
      "Collecting numpy>=1.23.2 (from pandas)\n",
      "  Downloading numpy-2.2.4-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "     ---------------------------------------- 0.0/60.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 60.8/60.8 kB 3.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\estudios_extra\\big_data_python\\aplicando_web_scraping\\venv_scrap\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting Twisted>=21.7.0 (from scrapy)\n",
      "  Downloading twisted-24.11.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting cryptography>=37.0.0 (from scrapy)\n",
      "  Downloading cryptography-44.0.2-cp39-abi3-win_amd64.whl.metadata (5.7 kB)\n",
      "Collecting cssselect>=0.9.1 (from scrapy)\n",
      "  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting itemloaders>=1.0.1 (from scrapy)\n",
      "  Downloading itemloaders-1.3.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting parsel>=1.5.0 (from scrapy)\n",
      "  Downloading parsel-1.10.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pyOpenSSL>=22.0.0 (from scrapy)\n",
      "  Downloading pyOpenSSL-25.0.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting queuelib>=1.4.2 (from scrapy)\n",
      "  Downloading queuelib-1.8.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting service-identity>=18.1.0 (from scrapy)\n",
      "  Downloading service_identity-24.2.0-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting w3lib>=1.17.0 (from scrapy)\n",
      "  Downloading w3lib-2.3.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting zope.interface>=5.1.0 (from scrapy)\n",
      "  Downloading zope.interface-7.2-cp311-cp311-win_amd64.whl.metadata (45 kB)\n",
      "     ---------------------------------------- 0.0/45.6 kB ? eta -:--:--\n",
      "     ---------------------------------------- 45.6/45.6 kB ? eta 0:00:00\n",
      "Collecting protego>=0.1.15 (from scrapy)\n",
      "  Downloading Protego-0.4.0-py2.py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting itemadapter>=0.1.0 (from scrapy)\n",
      "  Downloading itemadapter-0.11.0-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: packaging in d:\\estudios_extra\\big_data_python\\aplicando_web_scraping\\venv_scrap\\lib\\site-packages (from scrapy) (24.2)\n",
      "Collecting tldextract (from scrapy)\n",
      "  Downloading tldextract-5.2.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting defusedxml>=0.7.1 (from scrapy)\n",
      "  Downloading defusedxml-0.7.1-py2.py3-none-any.whl.metadata (32 kB)\n",
      "Collecting PyDispatcher>=2.0.5 (from scrapy)\n",
      "  Downloading PyDispatcher-2.0.7-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting trio~=0.17 (from selenium)\n",
      "  Using cached trio-0.29.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium)\n",
      "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting websocket-client~=1.8 (from selenium)\n",
      "  Using cached websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting cffi>=1.12 (from cryptography>=37.0.0->scrapy)\n",
      "  Using cached cffi-1.17.1-cp311-cp311-win_amd64.whl.metadata (1.6 kB)\n",
      "Collecting jmespath>=0.9.5 (from itemloaders>=1.0.1->scrapy)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Requirement already satisfied: six>=1.5 in d:\\estudios_extra\\big_data_python\\aplicando_web_scraping\\venv_scrap\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Collecting attrs>=19.1.0 (from service-identity>=18.1.0->scrapy)\n",
      "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting pyasn1 (from service-identity>=18.1.0->scrapy)\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting pyasn1-modules (from service-identity>=18.1.0->scrapy)\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting sortedcontainers (from trio~=0.17->selenium)\n",
      "  Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Collecting outcome (from trio~=0.17->selenium)\n",
      "  Using cached outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting sniffio>=1.3.0 (from trio~=0.17->selenium)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium)\n",
      "  Using cached wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting automat>=24.8.0 (from Twisted>=21.7.0->scrapy)\n",
      "  Downloading Automat-24.8.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting constantly>=15.1 (from Twisted>=21.7.0->scrapy)\n",
      "  Downloading constantly-23.10.4-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting hyperlink>=17.1.1 (from Twisted>=21.7.0->scrapy)\n",
      "  Downloading hyperlink-21.0.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting incremental>=24.7.0 (from Twisted>=21.7.0->scrapy)\n",
      "  Downloading incremental-24.7.2-py3-none-any.whl.metadata (8.1 kB)\n",
      "Collecting pysocks!=1.5.7,<2.0,>=1.5.6 (from urllib3[socks]<3,>=1.26->selenium)\n",
      "  Using cached PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: setuptools in d:\\estudios_extra\\big_data_python\\aplicando_web_scraping\\venv_scrap\\lib\\site-packages (from zope.interface>=5.1.0->scrapy) (65.5.0)\n",
      "Collecting requests-file>=1.4 (from tldextract->scrapy)\n",
      "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting filelock>=3.0.8 (from tldextract->scrapy)\n",
      "  Downloading filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting pycparser (from cffi>=1.12->cryptography>=37.0.0->scrapy)\n",
      "  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading beautifulsoup4-4.13.3-py3-none-any.whl (186 kB)\n",
      "   ---------------------------------------- 0.0/186.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 186.0/186.0 kB ? eta 0:00:00\n",
      "Using cached pandas-2.2.3-cp311-cp311-win_amd64.whl (11.6 MB)\n",
      "Downloading Scrapy-2.12.0-py2.py3-none-any.whl (311 kB)\n",
      "   ---------------------------------------- 0.0/311.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 311.2/311.2 kB ? eta 0:00:00\n",
      "Using cached selenium-4.31.0-py3-none-any.whl (9.4 MB)\n",
      "Downloading lxml-5.3.2-cp311-cp311-win_amd64.whl (3.8 MB)\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 3.8/3.8 MB 81.0 MB/s eta 0:00:00\n",
      "Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Using cached charset_normalizer-3.4.1-cp311-cp311-win_amd64.whl (102 kB)\n",
      "Downloading cryptography-44.0.2-cp39-abi3-win_amd64.whl (3.2 MB)\n",
      "   ---------------------------------------- 0.0/3.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 3.2/3.2 MB 103.2 MB/s eta 0:00:00\n",
      "Downloading cssselect-1.3.0-py3-none-any.whl (18 kB)\n",
      "Downloading defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading itemadapter-0.11.0-py3-none-any.whl (11 kB)\n",
      "Downloading itemloaders-1.3.2-py3-none-any.whl (12 kB)\n",
      "Downloading numpy-2.2.4-cp311-cp311-win_amd64.whl (12.9 MB)\n",
      "   ---------------------------------------- 0.0/12.9 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 4.5/12.9 MB 140.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 9.7/12.9 MB 124.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.9/12.9 MB 108.8 MB/s eta 0:00:00\n",
      "Downloading parsel-1.10.0-py2.py3-none-any.whl (17 kB)\n",
      "Downloading Protego-0.4.0-py2.py3-none-any.whl (8.6 kB)\n",
      "Downloading PyDispatcher-2.0.7-py3-none-any.whl (12 kB)\n",
      "Downloading pyOpenSSL-25.0.0-py3-none-any.whl (56 kB)\n",
      "   ---------------------------------------- 0.0/56.5 kB ? eta -:--:--\n",
      "   ---------------------------------------- 56.5/56.5 kB ? eta 0:00:00\n",
      "Downloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "   ---------------------------------------- 0.0/509.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 509.2/509.2 kB 33.3 MB/s eta 0:00:00\n",
      "Downloading queuelib-1.8.0-py3-none-any.whl (13 kB)\n",
      "Downloading service_identity-24.2.0-py3-none-any.whl (11 kB)\n",
      "Downloading soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Using cached trio-0.29.0-py3-none-any.whl (492 kB)\n",
      "Downloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
      "Downloading twisted-24.11.0-py3-none-any.whl (3.2 MB)\n",
      "   ---------------------------------------- 0.0/3.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 3.2/3.2 MB 99.3 MB/s eta 0:00:00\n",
      "Downloading tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "   ---------------------------------------- 0.0/347.8 kB ? eta -:--:--\n",
      "   --------------------------------------- 347.8/347.8 kB 22.5 MB/s eta 0:00:00\n",
      "Downloading urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "   ---------------------------------------- 0.0/128.7 kB ? eta -:--:--\n",
      "   ---------------------------------------- 128.7/128.7 kB 7.4 MB/s eta 0:00:00\n",
      "Downloading w3lib-2.3.1-py3-none-any.whl (21 kB)\n",
      "Using cached websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "Downloading zope.interface-7.2-cp311-cp311-win_amd64.whl (212 kB)\n",
      "   ---------------------------------------- 0.0/212.3 kB ? eta -:--:--\n",
      "   --------------------------------------- 212.3/212.3 kB 13.5 MB/s eta 0:00:00\n",
      "Downloading tldextract-5.2.0-py3-none-any.whl (106 kB)\n",
      "   ---------------------------------------- 0.0/106.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 106.3/106.3 kB 6.0 MB/s eta 0:00:00\n",
      "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "   ---------------------------------------- 0.0/63.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 63.8/63.8 kB ? eta 0:00:00\n",
      "Downloading Automat-24.8.1-py3-none-any.whl (42 kB)\n",
      "   ---------------------------------------- 0.0/42.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 42.6/42.6 kB ? eta 0:00:00\n",
      "Using cached cffi-1.17.1-cp311-cp311-win_amd64.whl (181 kB)\n",
      "Downloading constantly-23.10.4-py3-none-any.whl (13 kB)\n",
      "Downloading filelock-3.18.0-py3-none-any.whl (16 kB)\n",
      "Downloading hyperlink-21.0.0-py2.py3-none-any.whl (74 kB)\n",
      "   ---------------------------------------- 0.0/74.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 74.6/74.6 kB 4.0 MB/s eta 0:00:00\n",
      "Downloading incremental-24.7.2-py3-none-any.whl (20 kB)\n",
      "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Using cached outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Using cached PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Downloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Using cached pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "   ---------------------------------------- 0.0/181.3 kB ? eta -:--:--\n",
      "   ---------------------------------------- 181.3/181.3 kB ? eta 0:00:00\n",
      "Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Installing collected packages: sortedcontainers, pytz, PyDispatcher, zope.interface, websocket-client, w3lib, urllib3, tzdata, soupsieve, sniffio, queuelib, pysocks, pycparser, pyasn1, protego, numpy, lxml, jmespath, itemadapter, incremental, idna, h11, filelock, defusedxml, cssselect, constantly, charset-normalizer, certifi, automat, attrs, wsproto, requests, pyasn1-modules, parsel, pandas, outcome, hyperlink, cffi, beautifulsoup4, Twisted, trio, requests-file, itemloaders, cryptography, trio-websocket, tldextract, service-identity, pyOpenSSL, selenium, scrapy\n",
      "Successfully installed PyDispatcher-2.0.7 Twisted-24.11.0 attrs-25.3.0 automat-24.8.1 beautifulsoup4-4.13.3 certifi-2025.1.31 cffi-1.17.1 charset-normalizer-3.4.1 constantly-23.10.4 cryptography-44.0.2 cssselect-1.3.0 defusedxml-0.7.1 filelock-3.18.0 h11-0.14.0 hyperlink-21.0.0 idna-3.10 incremental-24.7.2 itemadapter-0.11.0 itemloaders-1.3.2 jmespath-1.0.1 lxml-5.3.2 numpy-2.2.4 outcome-1.3.0.post0 pandas-2.2.3 parsel-1.10.0 protego-0.4.0 pyOpenSSL-25.0.0 pyasn1-0.6.1 pyasn1-modules-0.4.2 pycparser-2.22 pysocks-1.7.1 pytz-2025.2 queuelib-1.8.0 requests-2.32.3 requests-file-2.1.0 scrapy-2.12.0 selenium-4.31.0 service-identity-24.2.0 sniffio-1.3.1 sortedcontainers-2.4.0 soupsieve-2.6 tldextract-5.2.0 trio-0.29.0 trio-websocket-0.12.2 tzdata-2025.2 urllib3-2.4.0 w3lib-2.3.1 websocket-client-1.8.0 wsproto-1.2.0 zope.interface-7.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "  pip install requests beautifulsoup4 pandas scrapy selenium lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e1bbec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip freeze > requirements.txt #Archivo notas de los paquetes instalados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e867281f",
   "metadata": {},
   "source": [
    "### Paso 1 (define ruta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d38947",
   "metadata": {},
   "source": [
    "La ruta de trabajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49ee2092",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\Estudios_extra\\\\Big_Data_Python\\\\Aplicando_WEB_SCRAPING'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os #Definir el directorio de trabajo\n",
    "os.chdir(r'D:\\Estudios_extra\\Big_Data_Python\\Aplicando_WEB_SCRAPING')\n",
    "os.getcwd() #conocer el directorio de trabajo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa6427fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options  # Importación correcta de Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ac92ae",
   "metadata": {},
   "source": [
    "## Entrar al link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58e92e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mejorar la configuración del navegador\n",
    "options = Options()\n",
    "options.add_argument(\"--start-maximized\")  # Maximizar ventana\n",
    "options.add_argument(\"--disable-notifications\")  # Desactivar notificaciones\n",
    "options.add_argument(\"--disable-popup-blocking\")  # Desactivar bloqueo de popups\n",
    "options.add_argument(\"--disable-infobars\")  # Desactivar infobars\n",
    "# options.add_argument(\"--headless\")  # Modo sin interfaz (descomentar si no necesitas ver el navegador)\n",
    "\n",
    "service = Service(ChromeDriverManager().install())\n",
    "\n",
    "# Inicializar el driver\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "driver.get(\"https://siea.midagri.gob.pe/portal/calendario/#\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1a39f1",
   "metadata": {},
   "source": [
    "## Ingresa cultivo y Clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbdbd539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar Maiz Amarillo Duro\n",
    "WebDriverWait(driver, 20).until(\n",
    "    EC.element_to_be_clickable((By.CSS_SELECTOR, \"span.select2-selection\"))\n",
    ").click()\n",
    "\n",
    "# Ingresar búsqueda\n",
    "search_box = WebDriverWait(driver, 10).until(\n",
    "    EC.visibility_of_element_located((By.CSS_SELECTOR, \"input.select2-search__field\"))\n",
    ")\n",
    "search_box.send_keys(\"Maiz Amarillo Duro\", Keys.RETURN)\n",
    "\n",
    "# Hacer click en el botón \"cosecha\"\n",
    "WebDriverWait(driver, 10).until(\n",
    "    EC.element_to_be_clickable((By.ID, \"btnCosecha\"))\n",
    ").click()\n",
    "\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b291a1",
   "metadata": {},
   "source": [
    "## Busca el departamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2cf61ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Configuración inicial\n",
    "    time.sleep(2)\n",
    "    action = ActionChains(driver)\n",
    "    departments = driver.find_elements(By.CSS_SELECTOR, \"path.highcharts-point\")\n",
    "    \n",
    "    # Buscar Lima en los departamentos\n",
    "    for department in departments:\n",
    "        try:\n",
    "            action.move_to_element(department).perform()\n",
    "            tooltip = driver.find_element(By.CSS_SELECTOR, \".highcharts-tooltip\")\n",
    "            \n",
    "            if \"Moquegua\" in tooltip.text.strip():\n",
    "                # Intentar hacer clic usando diferentes métodos\n",
    "                try:\n",
    "                    department.click()\n",
    "                except:\n",
    "                    try:\n",
    "                        action.click(department).perform()\n",
    "                    except:\n",
    "                        driver.execute_script(\"arguments[0].dispatchEvent(new MouseEvent('click', {bubbles: true}));\", department)\n",
    "                break\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516f490f",
   "metadata": {},
   "source": [
    "### Busca provincia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b631c1c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Configuración inicial\n",
    "    time.sleep(2)\n",
    "    action = ActionChains(driver)\n",
    "    departments = driver.find_elements(By.CSS_SELECTOR, \"path.highcharts-point\")\n",
    "    \n",
    "    # Buscar Lima en los departamentos\n",
    "    for department in departments:\n",
    "        try:\n",
    "            action.move_to_element(department).perform()\n",
    "            tooltip = driver.find_element(By.CSS_SELECTOR, \".highcharts-tooltip\")\n",
    "            \n",
    "            if \"Mariscal Nieto\" in tooltip.text.strip():\n",
    "                # Intentar hacer clic usando diferentes métodos\n",
    "                try:\n",
    "                    department.click()\n",
    "                except:\n",
    "                    try:\n",
    "                        action.click(department).perform()\n",
    "                    except:\n",
    "                        driver.execute_script(\"arguments[0].dispatchEvent(new MouseEvent('click', {bubbles: true}));\", department)\n",
    "                break\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71d4be4",
   "metadata": {},
   "source": [
    "## Click \"Regresar\" para volver al inicio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657d2f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:    \n",
    "    # Buscar el botón Regresar usando el selector específico de SVG\n",
    "    regresar_button = WebDriverWait(driver, 10).until(\n",
    "        EC.element_to_be_clickable((By.CSS_SELECTOR, \".highcharts-button-box\"))\n",
    "    )\n",
    "    \n",
    "    # Usar ActionChains para mover el mouse y hacer click en Regresar\n",
    "    action = ActionChains(driver)\n",
    "    action.move_to_element(regresar_button).click().perform()\n",
    "    \n",
    "    # Esperar un momento para que la página se actualice después de hacer clic en Regresar\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Hacer click en el botón \"Cosechas\" usando el ID correcto\n",
    "    try:\n",
    "        # Primer intento: buscar por ID\n",
    "        cosecha_button = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.ID, \"btnCosecha\"))\n",
    "        )\n",
    "        cosecha_button.click()\n",
    "    except Exception as cosecha_error:\n",
    "        print(f\"Error al buscar por ID: {cosecha_error}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error al hacer click en Regresar: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0714c012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se encontraron 10 barras visibles de 12 totales\n",
      "Analizando gráfico: Departamento: Arequipa\n",
      "Se encontraron 12 etiquetas en el eje X\n",
      "Etiqueta: Ene, Posición X: 1044.0286865234375\n",
      "Etiqueta: Feb, Posición X: 1084.78125\n",
      "Etiqueta: Mar, Posición X: 1125.2056884765625\n",
      "Etiqueta: Abr, Posición X: 1165.6146240234375\n",
      "Etiqueta: May, Posición X: 1205.0390625\n",
      "Etiqueta: Jun, Posición X: 1247.1119384765625\n",
      "Etiqueta: Jul, Posición X: 1289.5364990234375\n",
      "Etiqueta: Ago, Posición X: 1325.9453125\n",
      "Etiqueta: Set, Posición X: 1369.0338134765625\n",
      "Etiqueta: Oct, Posición X: 1409.1224365234375\n",
      "Etiqueta: Nov, Posición X: 1448.203125\n",
      "Etiqueta: Dic, Posición X: 1490.619873046875\n",
      "Barra 0 sin altura significativa o sin mes asociado\n",
      "Mes Feb: Porcentaje=2.5%, TM=4.0, Altura=28.0\n",
      "Mes Mar: Porcentaje=2.6%, TM=35.0, Altura=29.0\n",
      "Mes Abr: Porcentaje=2.6%, TM=16.0, Altura=29.0\n",
      "Mes May: Porcentaje=4.8%, TM=15.0, Altura=53.0\n",
      "Mes Jun: Porcentaje=20.2%, TM=100.0, Altura=223.0\n",
      "Mes Jul: Porcentaje=29.8%, TM=167.0, Altura=330.0\n",
      "Mes Ago: Porcentaje=29.8%, TM=1.0, Altura=330.0\n",
      "Mes Set: Porcentaje=7.3%, TM=156.0, Altura=81.0\n",
      "Mes Oct: Porcentaje=0.2%, TM=14.0, Altura=2.0\n",
      "Barra 10 sin altura significativa o sin mes asociado\n",
      "Mes Dic: Porcentaje=0.2%, TM=2.0, Altura=2.0\n",
      "\n",
      "Resultados finales ordenados por mes:\n",
      "Mes | Porcentaje | TM | Altura\n",
      "----|------------|----|---------\n",
      "Ene | N/A | N/A | N/A\n",
      "Feb | 2.5% | 4.0 | 28.0\n",
      "Mar | 2.6% | 35.0 | 29.0\n",
      "Abr | 2.6% | 16.0 | 29.0\n",
      "May | 4.8% | 15.0 | 53.0\n",
      "Jun | 20.2% | 100.0 | 223.0\n",
      "Jul | 29.8% | 167.0 | 330.0\n",
      "Ago | 29.8% | 1.0 | 330.0\n",
      "Set | 7.3% | 156.0 | 81.0\n",
      "Oct | 0.2% | 14.0 | 2.0\n",
      "Nov | N/A | N/A | N/A\n",
      "Dic | 0.2% | 2.0 | 2.0\n"
     ]
    }
   ],
   "source": [
    "# # Lista para almacenar los datos extraídos\n",
    "# datos_mensuales = []\n",
    "\n",
    "# # Meses del año\n",
    "# meses = ['Ene', 'Feb', 'Mar', 'Abr', 'May', 'Jun', 'Jul', 'Ago', 'Set', 'Oct', 'Nov', 'Dic']\n",
    "\n",
    "# try:\n",
    "#     # Esperar a que cargue el gráfico\n",
    "#     WebDriverWait(driver, 10).until(\n",
    "#         EC.presence_of_all_elements_located((By.CSS_SELECTOR, \".highcharts-column-series .highcharts-point\"))\n",
    "#     )\n",
    "    \n",
    "#     # Buscar las barras del gráfico que tienen altura visible\n",
    "#     barras = driver.find_elements(By.CSS_SELECTOR, \".highcharts-column-series .highcharts-point\")\n",
    "    \n",
    "#     # Filtrar solo barras visibles (con altura > 0)\n",
    "#     barras_visibles = []\n",
    "#     for barra in barras:\n",
    "#         altura = barra.get_attribute(\"height\")\n",
    "#         if altura and float(altura) > 0:\n",
    "#             barras_visibles.append(barra)\n",
    "    \n",
    "#     print(f\"Se encontraron {len(barras_visibles)} barras visibles de {len(barras)} totales\")\n",
    "    \n",
    "#     # Obtener el título del gráfico para validar\n",
    "#     titulo = driver.find_element(By.CSS_SELECTOR, \".highcharts-title\").text\n",
    "#     print(f\"Analizando gráfico: {titulo}\")\n",
    "    \n",
    "#     # Crear diccionario para almacenar datos por mes\n",
    "#     datos_por_mes = {mes: {\"porcentaje\": None, \"tm\": None} for mes in meses}\n",
    "    \n",
    "#     # Mover el cursor a cada etiqueta del eje X para determinar qué mes es cada posición\n",
    "#     etiquetas_x = driver.find_elements(By.CSS_SELECTOR, \".highcharts-xaxis-labels text\")\n",
    "#     print(f\"Se encontraron {len(etiquetas_x)} etiquetas en el eje X\")\n",
    "    \n",
    "#     # Mapear las etiquetas con sus posiciones\n",
    "#     meses_posiciones = {}\n",
    "#     for etiqueta in etiquetas_x:\n",
    "#         texto = etiqueta.text\n",
    "#         if texto in meses:\n",
    "#             pos_x = etiqueta.rect['x']\n",
    "#             meses_posiciones[texto] = pos_x\n",
    "#             print(f\"Etiqueta: {texto}, Posición X: {pos_x}\")\n",
    "    \n",
    "#     # Analizar cada barra y asociarla con el mes correcto\n",
    "#     for i, barra in enumerate(barras):\n",
    "#         try:\n",
    "#             # Obtener la posición X central de la barra\n",
    "#             pos_x_barra = barra.rect['x'] + (barra.rect['width'] / 2)\n",
    "#             altura = float(barra.get_attribute(\"height\") or 0)\n",
    "            \n",
    "#             # Encontrar el mes más cercano a esta posición X\n",
    "#             mes_cercano = None\n",
    "#             menor_distancia = float('inf')\n",
    "            \n",
    "#             for mes, pos_x in meses_posiciones.items():\n",
    "#                 distancia = abs(pos_x - pos_x_barra)\n",
    "#                 if distancia < menor_distancia:\n",
    "#                     menor_distancia = distancia\n",
    "#                     mes_cercano = mes\n",
    "            \n",
    "#             # Solo procesar si la barra tiene altura (es visible)\n",
    "#             if altura > 0 and mes_cercano:\n",
    "#                 # Mover el cursor a la barra para mostrar el tooltip\n",
    "#                 ActionChains(driver).move_to_element(barra).perform()\n",
    "#                 time.sleep(0.5)  # Esperar a que aparezca el tooltip\n",
    "                \n",
    "#                 # Intentar obtener el texto del tooltip\n",
    "#                 tooltip_elementos = driver.find_elements(By.CSS_SELECTOR, \".highcharts-tooltip text, .highcharts-tooltip-box + text\")\n",
    "                \n",
    "#                 tooltip_texto = \"\"\n",
    "#                 for elem in tooltip_elementos:\n",
    "#                     texto = elem.text\n",
    "#                     if texto and (\"%\" in texto or \"tm:\" in texto.lower()):\n",
    "#                         tooltip_texto = texto\n",
    "#                         break\n",
    "                \n",
    "#                 # Si no se encontró con selectores específicos, usar JavaScript\n",
    "#                 if not tooltip_texto:\n",
    "#                     tooltip_texto = driver.execute_script(\"\"\"\n",
    "#                         const textos = Array.from(document.querySelectorAll('.highcharts-tooltip text tspan'));\n",
    "#                         return textos.map(t => t.textContent).join('\\\\n');\n",
    "#                     \"\"\")\n",
    "                \n",
    "#                 # Extraer porcentaje y tm del tooltip\n",
    "#                 import re\n",
    "#                 porcentaje = None\n",
    "#                 tm = None\n",
    "                \n",
    "#                 # Patrones para extraer porcentaje y tm\n",
    "#                 porcentaje_match = re.search(r'(\\d+\\.?\\d*)\\s*%', tooltip_texto)\n",
    "#                 tm_match = re.search(r'tm:\\s*(\\d+\\.?\\d*)', tooltip_texto)\n",
    "                \n",
    "#                 if porcentaje_match:\n",
    "#                     porcentaje = float(porcentaje_match.group(1))\n",
    "                \n",
    "#                 if tm_match:\n",
    "#                     tm = float(tm_match.group(1))\n",
    "                \n",
    "#                 # Guardar datos para este mes\n",
    "#                 if mes_cercano:\n",
    "#                     datos_por_mes[mes_cercano] = {\n",
    "#                         \"porcentaje\": porcentaje,\n",
    "#                         \"tm\": tm,\n",
    "#                         \"altura\": altura,\n",
    "#                         \"tooltip\": tooltip_texto\n",
    "#                     }\n",
    "                    \n",
    "#                     print(f\"Mes {mes_cercano}: Porcentaje={porcentaje}%, TM={tm}, Altura={altura}\")\n",
    "                \n",
    "#             else:\n",
    "#                 print(f\"Barra {i} sin altura significativa o sin mes asociado\")\n",
    "                \n",
    "#         except Exception as e:\n",
    "#             print(f\"Error al procesar barra {i}: {e}\")\n",
    "    \n",
    "#     # Convertir el diccionario a una lista ordenada por los meses\n",
    "#     datos_mensuales = []\n",
    "#     for mes in meses:\n",
    "#         if mes in datos_por_mes:\n",
    "#             datos = datos_por_mes[mes]\n",
    "#             datos_mensuales.append({\n",
    "#                 \"mes\": mes,\n",
    "#                 \"porcentaje\": datos.get(\"porcentaje\"),\n",
    "#                 \"tm\": datos.get(\"tm\"),\n",
    "#                 \"altura\": datos.get(\"altura\"),\n",
    "#                 \"tooltip\": datos.get(\"tooltip\", \"\")\n",
    "#             })\n",
    "    \n",
    "#     # Mostrar resultados finales\n",
    "#     print(\"\\nResultados finales ordenados por mes:\")\n",
    "#     print(\"Mes | Porcentaje | TM | Altura\")\n",
    "#     print(\"----|------------|----|---------\")\n",
    "#     for dato in datos_mensuales:\n",
    "#         porcentaje_str = f\"{dato['porcentaje']}%\" if dato['porcentaje'] is not None else \"N/A\"\n",
    "#         tm_str = str(dato['tm']) if dato['tm'] is not None else \"N/A\"\n",
    "#         altura_str = str(dato['altura']) if dato['altura'] is not None else \"N/A\"\n",
    "#         print(f\"{dato['mes']} | {porcentaje_str} | {tm_str} | {altura_str}\")\n",
    "\n",
    "# except Exception as e:\n",
    "#     print(f\"Error general: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c55e68e",
   "metadata": {},
   "source": [
    "## Extraer los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f8b2cc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Datos en formato matriz (DataFrame):\n",
      "   departamento             cultivo  mes  porcentaje     tm\n",
      "0  Departamento  Maiz Amarillo Duro  Abr         1.9    NaN\n",
      "1  Departamento  Maiz Amarillo Duro  May        12.7  360.0\n",
      "2  Departamento  Maiz Amarillo Duro  Jun        27.3    1.0\n",
      "3  Departamento  Maiz Amarillo Duro  Jul        38.4    1.0\n",
      "4  Departamento  Maiz Amarillo Duro  Ago        17.4  612.0\n",
      "5  Departamento  Maiz Amarillo Duro  Set         2.3    NaN\n",
      "6  Departamento  Maiz Amarillo Duro  Nov         0.1    NaN\n"
     ]
    }
   ],
   "source": [
    "# Importar la función\n",
    "from scraper.utils.extractores import extraer_datos_grafico_calendario\n",
    "import pandas as pd\n",
    "\n",
    "# Usar la función\n",
    "datos = extraer_datos_grafico_calendario(driver)\n",
    "\n",
    "# Convertir a DataFrame (matriz)\n",
    "if datos and 'datos_mensuales' in datos:\n",
    "    # Crear DataFrame a partir de los datos mensuales\n",
    "    df = pd.DataFrame(datos['datos_mensuales'])\n",
    "    \n",
    "    # Añadir información del departamento y cultivo\n",
    "    df['departamento'] = datos['departamento']\n",
    "    df['cultivo'] = datos['cultivo']\n",
    "    \n",
    "    # Reordenar columnas si lo prefieres\n",
    "    df = df[['departamento', 'cultivo', 'mes', 'porcentaje', 'tm']]\n",
    "    \n",
    "    # Mostrar como tabla\n",
    "    print(\"\\nDatos en formato matriz (DataFrame):\")\n",
    "    print(df)\n",
    "    \n",
    "    # Opcionalmente guardar en CSV\n",
    "    # df.to_csv(f\"{datos['departamento']}_{datos['cultivo'].replace(' ', '_')}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_scrap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
